// ============================================================================
// OpenAI to NVIDIA NIM Proxy (Long Chat + Peak Hour Optimized)
// Optimized for lengthy conversations and automatic DeepSeek model fallback
// ============================================================================

const express = require(â€œexpressâ€);
const cors = require(â€œcorsâ€);
const axios = require(â€œaxiosâ€);

const app = express();
const PORT = process.env.PORT || 3000;

// ============================================================================
// MIDDLEWARE CONFIGURATION
// ============================================================================

app.use(cors({
origin: â€˜*â€™,
methods: [â€˜GETâ€™, â€˜POSTâ€™, â€˜OPTIONSâ€™],
allowedHeaders: [â€˜Content-Typeâ€™, â€˜Authorizationâ€™],
credentials: true
}));

app.use(express.json({ limit: â€œ100mbâ€ })); // Support EXTREMELY long chat histories (10k+ messages)

// ============================================================================
// API CONFIGURATION
// ============================================================================

const NIM_API_BASE = â€œhttps://integrate.api.nvidia.com/v1â€;
const API_KEY = process.env.NIM_API_KEY;
const PRIMARY_MODEL = process.env.NIM_MODEL || â€œdeepseek-ai/deepseek-v3.2â€; // Changed back to V3.2

// Fallback models - V3.1 first, then others
const FALLBACK_MODELS = [
â€œdeepseek-ai/deepseek-v3.1â€,                  // First fallback
â€œdeepseek-ai/deepseek-r1-distill-qwen-32bâ€,   // Distilled version (faster)
â€œdeepseek-ai/deepseek-r1-distill-qwen-14bâ€,   // Even smaller/faster
â€œdeepseek-ai/deepseek-v3.1-terminusâ€          // Alternative V3.1
];

const STRICT_MODE = false; // Set to true to disable fallbacks

// ============================================================================
// CONVERSATION TRUNCATION SETTINGS
// ============================================================================

// Adaptive truncation - optimized for speed
const ENABLE_SMART_TRUNCATION = true;
const TRUNCATION_TIERS = {
// conversations under 100 messages: send everything
small: { threshold: 100, keep: 100, keepFirst: 0 },

// 100-300 messages: send 120 most important
medium: { threshold: 300, keep: 120, keepFirst: 10 },

// 300-1000 messages: send 150 most important  
large: { threshold: 1000, keep: 150, keepFirst: 15 },

// 1000+ messages: send 180 most important (faster!)
huge: { threshold: Infinity, keep: 180, keepFirst: 20 }
};

// ============================================================================
// STATE TRACKING
// ============================================================================

let lastHit = null;
let failedAttempts = 0;
let currentModel = PRIMARY_MODEL;

function recordHit(req) {
lastHit = {
time: new Date().toISOString(),
method: req.method,
path: req.path,
url: req.url,
};
console.log(â€œğŸ”µ HIT:â€, lastHit);
}

// ============================================================================
// INFORMATION ENDPOINTS
// ============================================================================

// Root endpoint
app.get(â€/â€, (req, res) => {
recordHit(req);
res.type(â€œtextâ€).send(â€œâœ… Proxy running! Try /health or POST to /v1/chat/completionsâ€);
});

// Health check endpoint
app.get(â€/healthâ€, (req, res) => {
recordHit(req);
res.json({
status: â€œokâ€,
service: â€œOpenAI to NVIDIA NIM Proxyâ€,
primaryModel: PRIMARY_MODEL,
currentModel: currentModel,
failedAttempts: failedAttempts,
fallbackModels: FALLBACK_MODELS.length,
hasNimKey: !!API_KEY
});
});

// Whoami debug endpoint
app.get(â€/whoamiâ€, (req, res) => {
recordHit(req);
res.json({
lastHit,
primaryModel: PRIMARY_MODEL,
currentModel: currentModel,
failedAttempts: failedAttempts,
fallbackModels: FALLBACK_MODELS,
hasNimKey: !!API_KEY
});
});

// Upstream models endpoint - shows all available NVIDIA models
app.get(â€/upstream/modelsâ€, async (req, res) => {
recordHit(req);

try {
if (!API_KEY) {
return res.status(500).json({
error: { message: â€œMissing NIM_API_KEYâ€ }
});
}

```
const response = await axios.get(`${NIM_API_BASE}/models`, {
  headers: { Authorization: `Bearer ${API_KEY}` },
  timeout: 60000,
});

res.json(response.data);
```

} catch (error) {
res.status(error.response?.status || 500).json(
error.response?.data || { message: error.message }
);
}
});

// v1 base route
app.get(â€/v1â€, (req, res) => {
recordHit(req);
res.json({
status: â€œokâ€,
message: â€œOpenAI-compatible API v1â€,
endpoints: [â€/v1/modelsâ€, â€œ/v1/chat/completionsâ€]
});
});

// OpenAI-style model list
app.get(â€/v1/modelsâ€, (req, res) => {
recordHit(req);
res.json({
object: â€œlistâ€,
data: [
{
id: â€œgpt-4â€,
object: â€œmodelâ€,
created: Date.now(),
owned_by: â€œproxyâ€
},
{
id: â€œgpt-4oâ€,
object: â€œmodelâ€,
created: Date.now(),
owned_by: â€œproxyâ€
},
{
id: â€œgpt-3.5-turboâ€,
object: â€œmodelâ€,
created: Date.now(),
owned_by: â€œproxyâ€
},
{
id: PRIMARY_MODEL,
object: â€œmodelâ€,
created: Date.now(),
owned_by: â€œproxyâ€
}
],
});
});

// ============================================================================
// SMART MESSAGE TRUNCATION - Handles 10k+ message conversations
// ============================================================================

function truncateMessages(messages) {
if (!ENABLE_SMART_TRUNCATION) {
return messages;
}

// Determine which tier based on conversation length
let tier;
if (messages.length < TRUNCATION_TIERS.small.threshold) {
tier = TRUNCATION_TIERS.small;
} else if (messages.length < TRUNCATION_TIERS.medium.threshold) {
tier = TRUNCATION_TIERS.medium;
} else if (messages.length < TRUNCATION_TIERS.large.threshold) {
tier = TRUNCATION_TIERS.large;
} else {
tier = TRUNCATION_TIERS.huge;
}

// If conversation is short enough, send everything
if (messages.length <= tier.keep) {
console.log(`âœ… Conversation size: ${messages.length} messages - sending all`);
return messages;
}

console.log(`âš ï¸ Long conversation detected: ${messages.length} messages`);
console.log(`ğŸ“‰ Truncating to ${tier.keep} messages (tier: ${tier.keepFirst} first + ${tier.keep - tier.keepFirst} recent)`);

// Keep first N messages (character intro, setting, etc)
const firstMessages = tier.keepFirst > 0 ? messages.slice(0, tier.keepFirst) : [];

// Keep most recent messages
const recentMessages = messages.slice(-(tier.keep - tier.keepFirst));

// Combine them
const truncated = [â€¦firstMessages, â€¦recentMessages];

console.log(`âœ… Truncated from ${messages.length} to ${truncated.length} messages`);
if (tier.keepFirst > 0) {
console.log(`   - First ${tier.keepFirst} messages (character context)`);
console.log(`   - Last ${recentMessages.length} messages (recent conversation)`);
} else {
console.log(`   - Last ${recentMessages.length} messages only`);
}

return truncated;
}

// ============================================================================
// SMART RETRY FUNCTION - Handles failures and auto-switches DeepSeek models
// ============================================================================

async function makeNvidiaRequest(messages, temperature, max_tokens, stream, attemptNum = 0) {
const modelToUse = attemptNum === 0 ? currentModel : FALLBACK_MODELS[attemptNum - 1];

// All models exhausted
if (!modelToUse) {
throw new Error(â€œAll DeepSeek fallback models exhaustedâ€);
}

console.log(`ğŸ¯ Attempt ${attemptNum + 1}: Using model ${modelToUse}`);

try {
const response = await axios.post(
`${NIM_API_BASE}/chat/completions`,
{
model: modelToUse,
messages: messages,
temperature: temperature,
max_tokens: max_tokens,
stream: stream
},
{
headers: {
Authorization: `Bearer ${API_KEY}`,
â€œContent-Typeâ€: â€œapplication/jsonâ€
},
timeout: 600000, // 10 minutes for EXTREMELY long chats (10k+ messages)
responseType: stream ? â€˜streamâ€™ : â€˜jsonâ€™,
validateStatus: (status) => status < 500 // Donâ€™t throw on 4xx
}
);

```
// Success! Reset failure counter and update current model
if (response.status === 200) {
  failedAttempts = 0;
  
  if (modelToUse !== currentModel) {
    console.log(`âœ… Switched to ${modelToUse} due to better availability`);
    currentModel = modelToUse;
  }
  
  return response;
}

// 4xx errors (bad request, rate limit, model unavailable, etc)
if (response.status >= 400 && response.status < 500) {
  console.log(`âš ï¸ Model ${modelToUse} returned ${response.status}, trying fallback...`);
  
  // Rate limit or model unavailable - try next model
  if (attemptNum < FALLBACK_MODELS.length) {
    failedAttempts++;
    await new Promise(resolve => setTimeout(resolve, 1000)); // Wait 1 sec
    return makeNvidiaRequest(messages, temperature, max_tokens, stream, attemptNum + 1);
  }
  
  // All attempts failed
  throw { response };
}

throw { response };
```

} catch (error) {
// Network errors, timeouts, 5xx errors
console.log(`âŒ Model ${modelToUse} failed: ${error.message}`);

```
// Try next fallback model
if (attemptNum < FALLBACK_MODELS.length) {
  failedAttempts++;
  console.log(`ğŸ”„ Trying fallback model (attempt ${attemptNum + 2})...`);
  await new Promise(resolve => setTimeout(resolve, 2000)); // Wait 2 sec
  return makeNvidiaRequest(messages, temperature, max_tokens, stream, attemptNum + 1);
}

// All attempts exhausted
throw error;
```

}
}

// ============================================================================
// MAIN CHAT COMPLETION ENDPOINT
// ============================================================================

app.post(â€/v1/chat/completionsâ€, async (req, res) => {
recordHit(req);
console.log(â€œğŸ“¨ POST /v1/chat/completionsâ€);

try {
// Validate API key
if (!API_KEY) {
return res.status(500).json({
error: { message: â€œMissing NIM_API_KEYâ€ }
});
}

```
// Parse request body
const body = req.body || {};
const messages = Array.isArray(body.messages) ? body.messages : [];
const temperature = body.temperature ?? 0.7;
const max_tokens = Math.max(body.max_tokens ?? 12000, 200); // Higher default for long responses
const stream = body.stream || false;

// Log request details
const totalChars = JSON.stringify(messages).length;
console.log(`ğŸ“Š Received ${messages.length} messages, ${totalChars} chars total`);

// Smart truncation for extremely long conversations
const processedMessages = truncateMessages(messages);
const processedChars = JSON.stringify(processedMessages).length;

if (processedMessages.length < messages.length) {
  console.log(`ğŸ“¦ Sending ${processedMessages.length} messages, ${processedChars} chars to NVIDIA`);
}

// Make request with auto-retry and fallback
const response = await makeNvidiaRequest(processedMessages, temperature, max_tokens, stream);

// ========================================================================
// STREAMING RESPONSE
// ========================================================================

if (stream) {
  res.setHeader('Content-Type', 'text/event-stream');
  res.setHeader('Cache-Control', 'no-cache');
  res.setHeader('Connection', 'keep-alive');
  
  response.data.pipe(res);
  
  response.data.on('end', () => {
    console.log("âœ… Stream completed");
  });
  
  response.data.on('error', (err) => {
    console.error('âŒ Stream error:', err);
    res.end();
  });
  
  return;
}

// ========================================================================
// NON-STREAMING RESPONSE
// ========================================================================

const reply = response.data?.choices?.[0]?.message?.content || "";

const openaiResponse = {
  id: response.data?.id || `chatcmpl-${Date.now()}`,
  object: "chat.completion",
  created: response.data?.created || Math.floor(Date.now() / 1000),
  model: body.model || "gpt-3.5-turbo",
  choices: [
    { 
      index: 0, 
      message: { 
        role: "assistant", 
        content: reply || " "
      }, 
      finish_reason: response.data?.choices?.[0]?.finish_reason || "stop" 
    }
  ],
  usage: response.data?.usage || { 
    prompt_tokens: 10, 
    completion_tokens: 50, 
    total_tokens: 60 
  }
};

res.setHeader('Content-Type', 'application/json');
res.json(openaiResponse);

console.log(`âœ… Response sent (${reply.length} chars, model: ${currentModel})`);
```

} catch (error) {
// ========================================================================
// ERROR HANDLING
// ========================================================================

```
console.error("âŒ ALL ATTEMPTS FAILED");
console.error("âŒ ERROR:", error.message);
console.error("âŒ NVIDIA STATUS:", error.response?.status);
console.error("âŒ NVIDIA DATA:", JSON.stringify(error.response?.data));

// Handle undefined/empty NVIDIA responses
if (!error.response || !error.response.data) {
  console.error("âŒ NVIDIA returned undefined/empty response");
  return res.status(503).json({
    error: { 
      message: "All DeepSeek models temporarily unavailable. NVIDIA API may be overloaded. Try again in 1-2 minutes.",
      type: 'service_unavailable',
      code: 503,
      suggestion: "Peak hours detected. Consider trying again later."
    }
  });
}

// Rate limit specific message
if (error.response?.status === 429) {
  return res.status(429).json({
    error: { 
      message: "Rate limit exceeded on all DeepSeek models. Please wait 5-10 minutes.",
      type: 'rate_limit_error',
      code: 429,
      suggestion: "You've hit NVIDIA's free tier limit. Wait a bit or try during off-peak hours."
    }
  });
}

// Generic error response
res.status(error.response?.status || 500).json({
  error: error.response?.data || { 
    message: error.message || "Unknown error occurred",
    type: 'invalid_request_error',
    suggestion: "Try refreshing or waiting a moment"
  }
});
```

}
});

// ============================================================================
// ALTERNATIVE ROUTES FOR COMPATIBILITY
// ============================================================================

app.post(â€/v1/chat/completions/â€, (req, res, next) => {
req.url = â€œ/v1/chat/completionsâ€;
app.handle(req, res, next);
});

app.post(â€/chat/completionsâ€, (req, res, next) => {
req.url = â€œ/v1/chat/completionsâ€;
app.handle(req, res, next);
});

app.post(â€/â€, async (req, res) => {
recordHit(req);

// If it looks like a chat request, route it
if (req.body && req.body.messages) {
req.url = â€œ/v1/chat/completionsâ€;
return app.handle(req, res);
}

res.type(â€œtextâ€).send(â€œâœ… POST received! For chat, use /v1/chat/completionsâ€);
});

// ============================================================================
// CATCH-ALL 404 HANDLER
// ============================================================================

app.use((req, res) => {
recordHit(req);
console.log(â€œâŒ 404 - Route not found:â€, req.method, req.path);

res.status(404).json({
error: {
message: â€œRoute not foundâ€,
method: req.method,
path: req.path,
url: req.url
}
});
});

// ============================================================================
// SERVER STARTUP
// ============================================================================

app.listen(PORT, () => {
console.log(â€=â€.repeat(60));
console.log(â€œğŸš€ OpenAI to NVIDIA NIM Proxyâ€);
console.log(â€   Long Chat + Peak Hour Optimized + DeepSeek Fallbackâ€);
console.log(â€=â€.repeat(60));
console.log(`ğŸ“¡ Port:              ${PORT}`);
console.log(`ğŸ¤– Primary Model:     ${PRIMARY_MODEL}`);
console.log(`ğŸ”„ Fallback Models:   ${FALLBACK_MODELS.length} DeepSeek models`);
console.log(`ğŸ”‘ API Key:           ${API_KEY ? "âœ… Loaded" : "âŒ Missing"}`);
console.log(`ğŸ’¾ Max Request Size:  100MB (10k+ messages supported)`);
console.log(`â±ï¸  Request Timeout:   10 minutes`);
console.log(`ğŸ“‰ Smart Truncation:  ${ENABLE_SMART_TRUNCATION ? "ON (Adaptive)" : "OFF"}`);
console.log(`ğŸŒ Health Check:      http://localhost:${PORT}/health`);
console.log(â€=â€.repeat(60));
console.log(â€œğŸ“‹ Fallback Order:â€);
console.log(`   1. ${PRIMARY_MODEL} (primary)`);
FALLBACK_MODELS.forEach((model, i) => {
console.log(`   ${i + 2}. ${model}`);
});
console.log(â€=â€.repeat(60));
});
