// ============================================================================
// OpenAI to NVIDIA NIM Proxy (Free Tier with Anti-Analyzing Fix)
// Optimized for lengthy conversations and stops over-analyzing
// ============================================================================

const express = require(â€œexpressâ€);
const cors = require(â€œcorsâ€);
const axios = require(â€œaxiosâ€);

const app = express();
const PORT = process.env.PORT || 3000;

// ============================================================================
// MIDDLEWARE CONFIGURATION
// ============================================================================

app.use(cors({
origin: â€˜*â€™,
methods: [â€˜GETâ€™, â€˜POSTâ€™, â€˜OPTIONSâ€™],
allowedHeaders: [â€˜Content-Typeâ€™, â€˜Authorizationâ€™],
credentials: true
}));

app.use(express.json({ limit: â€œ100mbâ€ }));

// ============================================================================
// API CONFIGURATION
// ============================================================================

const NIM_API_BASE = â€œhttps://integrate.api.nvidia.com/v1â€;
const API_KEY = process.env.NIM_API_KEY;
const PRIMARY_MODEL = process.env.NIM_MODEL || â€œdeepseek-ai/deepseek-v3.2â€;

// Fallback models - DeepSeek family only
const FALLBACK_MODELS = [
â€œdeepseek-ai/deepseek-v3.1â€,
â€œdeepseek-ai/deepseek-r1-distill-qwen-32bâ€,
â€œdeepseek-ai/deepseek-r1-distill-qwen-14bâ€,
â€œdeepseek-ai/deepseek-v3.1-terminusâ€
];

const STRICT_MODE = false;

// ============================================================================
// CONVERSATION TRUNCATION SETTINGS
// ============================================================================

const ENABLE_SMART_TRUNCATION = true;
const TRUNCATION_TIERS = {
small: { threshold: 100, keep: 100, keepFirst: 0 },
medium: { threshold: 300, keep: 120, keepFirst: 10 },
large: { threshold: 1000, keep: 150, keepFirst: 15 },
huge: { threshold: Infinity, keep: 180, keepFirst: 20 }
};

// ============================================================================
// STATE TRACKING
// ============================================================================

let lastHit = null;
let failedAttempts = 0;
let currentModel = PRIMARY_MODEL;

function recordHit(req) {
lastHit = {
time: new Date().toISOString(),
method: req.method,
path: req.path,
url: req.url,
};
console.log(â€œğŸ”µ HIT:â€, lastHit);
}

// ============================================================================
// INFORMATION ENDPOINTS
// ============================================================================

app.get(â€/â€, (req, res) => {
recordHit(req);
res.type(â€œtextâ€).send(â€œâœ… Proxy running! Try /health or POST to /v1/chat/completionsâ€);
});

app.get(â€/healthâ€, (req, res) => {
recordHit(req);
res.json({
status: â€œokâ€,
service: â€œOpenAI to NVIDIA NIM Proxyâ€,
primaryModel: PRIMARY_MODEL,
currentModel: currentModel,
failedAttempts: failedAttempts,
fallbackModels: FALLBACK_MODELS.length,
hasNimKey: !!API_KEY,
antiAnalyzing: true
});
});

app.get(â€/whoamiâ€, (req, res) => {
recordHit(req);
res.json({
lastHit,
primaryModel: PRIMARY_MODEL,
currentModel: currentModel,
failedAttempts: failedAttempts,
fallbackModels: FALLBACK_MODELS,
hasNimKey: !!API_KEY
});
});

app.get(â€/upstream/modelsâ€, async (req, res) => {
recordHit(req);
try {
if (!API_KEY) {
return res.status(500).json({ error: { message: â€œMissing NIM_API_KEYâ€ } });
}
const r = await axios.get(`${NIM_API_BASE}/models`, {
headers: { Authorization: `Bearer ${API_KEY}` },
timeout: 60000,
});
res.json(r.data);
} catch (e) {
res.status(e.response?.status || 500).json(e.response?.data || { message: e.message });
}
});

app.get(â€/v1â€, (req, res) => {
recordHit(req);
res.json({
status: â€œokâ€,
message: â€œOpenAI-compatible API v1â€,
endpoints: [â€/v1/modelsâ€, â€œ/v1/chat/completionsâ€]
});
});

app.get(â€/v1/modelsâ€, (req, res) => {
recordHit(req);
res.json({
object: â€œlistâ€,
data: [
{ id: â€œgpt-4â€, object: â€œmodelâ€, created: Date.now(), owned_by: â€œproxyâ€ },
{ id: â€œgpt-4oâ€, object: â€œmodelâ€, created: Date.now(), owned_by: â€œproxyâ€ },
{ id: â€œgpt-3.5-turboâ€, object: â€œmodelâ€, created: Date.now(), owned_by: â€œproxyâ€ },
{ id: PRIMARY_MODEL, object: â€œmodelâ€, created: Date.now(), owned_by: â€œproxyâ€ }
],
});
});

// ============================================================================
// SMART MESSAGE TRUNCATION
// ============================================================================

function truncateMessages(messages) {
if (!ENABLE_SMART_TRUNCATION) {
return messages;
}

let tier;
if (messages.length < TRUNCATION_TIERS.small.threshold) {
tier = TRUNCATION_TIERS.small;
} else if (messages.length < TRUNCATION_TIERS.medium.threshold) {
tier = TRUNCATION_TIERS.medium;
} else if (messages.length < TRUNCATION_TIERS.large.threshold) {
tier = TRUNCATION_TIERS.large;
} else {
tier = TRUNCATION_TIERS.huge;
}

if (messages.length <= tier.keep) {
console.log(`âœ… Conversation size: ${messages.length} messages - sending all`);
return messages;
}

console.log(`âš ï¸ Long conversation detected: ${messages.length} messages`);
console.log(`ğŸ“‰ Truncating to ${tier.keep} messages (tier: ${tier.keepFirst} first + ${tier.keep - tier.keepFirst} recent)`);

const firstMessages = tier.keepFirst > 0 ? messages.slice(0, tier.keepFirst) : [];
const recentMessages = messages.slice(-(tier.keep - tier.keepFirst));
const truncated = [â€¦firstMessages, â€¦recentMessages];

console.log(`âœ… Truncated from ${messages.length} to ${truncated.length} messages`);
if (tier.keepFirst > 0) {
console.log(`   - First ${tier.keepFirst} messages (character context)`);
console.log(`   - Last ${recentMessages.length} messages (recent conversation)`);
} else {
console.log(`   - Last ${recentMessages.length} messages only`);
}

return truncated;
}

// ============================================================================
// SMART RETRY FUNCTION
// ============================================================================

async function makeNvidiaRequest(messages, temperature, max_tokens, stream, attemptNum = 0) {
const modelToUse = attemptNum === 0 ? currentModel : FALLBACK_MODELS[attemptNum - 1];

if (!modelToUse) {
throw new Error(â€œAll DeepSeek fallback models exhaustedâ€);
}

console.log(`ğŸ¯ Attempt ${attemptNum + 1}: Using model ${modelToUse}`);

try {
const response = await axios.post(
`${NIM_API_BASE}/chat/completions`,
{
model: modelToUse,
messages: messages,
temperature: temperature,
max_tokens: max_tokens,
stream: stream
},
{
headers: {
Authorization: `Bearer ${API_KEY}`,
â€œContent-Typeâ€: â€œapplication/jsonâ€
},
timeout: 600000,
responseType: stream ? â€˜streamâ€™ : â€˜jsonâ€™,
validateStatus: (status) => status < 500
}
);

```
if (response.status === 200) {
  failedAttempts = 0;
  
  if (modelToUse !== currentModel) {
    console.log(`âœ… Switched to ${modelToUse} due to better availability`);
    currentModel = modelToUse;
  }
  
  return response;
}

if (response.status >= 400 && response.status < 500) {
  console.log(`âš ï¸ Model ${modelToUse} returned ${response.status}, trying fallback...`);
  
  if (attemptNum < FALLBACK_MODELS.length) {
    failedAttempts++;
    await new Promise(resolve => setTimeout(resolve, 1000));
    return makeNvidiaRequest(messages, temperature, max_tokens, stream, attemptNum + 1);
  }
  
  throw { response };
}

throw { response };
```

} catch (error) {
console.log(`âŒ Model ${modelToUse} failed: ${error.message}`);

```
if (attemptNum < FALLBACK_MODELS.length) {
  failedAttempts++;
  console.log(`ğŸ”„ Trying fallback model (attempt ${attemptNum + 2})...`);
  await new Promise(resolve => setTimeout(resolve, 2000));
  return makeNvidiaRequest(messages, temperature, max_tokens, stream, attemptNum + 1);
}

throw error;
```

}
}

// ============================================================================
// MAIN CHAT COMPLETION ENDPOINT
// ============================================================================

app.post(â€/v1/chat/completionsâ€, async (req, res) => {
recordHit(req);
console.log(â€œğŸ“¨ POST /v1/chat/completionsâ€);

try {
if (!API_KEY) {
return res.status(500).json({ error: { message: â€œMissing NIM_API_KEYâ€ } });
}

```
const body = req.body || {};
let messages = Array.isArray(body.messages) ? body.messages : [];
const temperature = body.temperature ?? 0.7;
const requestedMaxTokens = body.max_tokens ?? 12000;
const max_tokens = Math.min(Math.max(requestedMaxTokens, 200), 8000);
const stream = body.stream || false;

// ğŸ”¥ ADD SYSTEM MESSAGE TO PREVENT OVER-ANALYZING
const hasSystemMessage = messages.some(msg => msg.role === "system");
if (!hasSystemMessage) {
  messages = [
    {
      role: "system",
      content: "Respond naturally and directly. Do not analyze or overthink. Answer concisely and stay in character."
    },
    ...messages
  ];
  console.log("âœ… Added anti-analyzing system message");
}

const totalChars = JSON.stringify(messages).length;
console.log(`ğŸ“Š Received ${messages.length} messages, ${totalChars} chars total`);

const processedMessages = truncateMessages(messages);
const processedChars = JSON.stringify(processedMessages).length;

if (processedMessages.length < messages.length) {
  console.log(`ğŸ“¦ Sending ${processedMessages.length} messages, ${processedChars} chars to NVIDIA`);
}

const response = await makeNvidiaRequest(processedMessages, temperature, max_tokens, stream);

if (stream) {
  res.setHeader('Content-Type', 'text/event-stream');
  res.setHeader('Cache-Control', 'no-cache');
  res.setHeader('Connection', 'keep-alive');
  
  response.data.pipe(res);
  
  response.data.on('end', () => {
    console.log("âœ… Stream completed");
  });
  
  response.data.on('error', (err) => {
    console.error('âŒ Stream error:', err);
    res.end();
  });
  
  return;
}

const reply = response.data?.choices?.[0]?.message?.content || "";

const openaiResponse = {
  id: response.data?.id || `chatcmpl-${Date.now()}`,
  object: "chat.completion",
  created: response.data?.created || Math.floor(Date.now() / 1000),
  model: body.model || "gpt-3.5-turbo",
  choices: [
    { 
      index: 0, 
      message: { 
        role: "assistant", 
        content: reply || " "
      }, 
      finish_reason: response.data?.choices?.[0]?.finish_reason || "stop"
    }
  ],
  usage: response.data?.usage || { 
    prompt_tokens: 10, 
    completion_tokens: 50, 
    total_tokens: 60 
  }
};

res.setHeader('Content-Type', 'application/json');
res.json(openaiResponse);

console.log(`âœ… Response sent (${reply.length} chars, model: ${currentModel})`);
```

} catch (error) {
console.error(â€œâŒ ALL ATTEMPTS FAILEDâ€);
console.error(â€œâŒ ERROR:â€, error.message);
console.error(â€œâŒ NVIDIA STATUS:â€, error.response?.status);
console.error(â€œâŒ NVIDIA DATA:â€, JSON.stringify(error.response?.data));

```
if (!error.response || !error.response.data) {
  return res.status(503).json({
    error: { 
      message: "All DeepSeek models temporarily unavailable. NVIDIA API may be overloaded. Try again in 1-2 minutes.",
      type: 'service_unavailable',
      code: 503
    }
  });
}

if (error.response?.status === 429) {
  return res.status(429).json({
    error: { 
      message: "Rate limit exceeded on all DeepSeek models. Please wait 5-10 minutes.",
      type: 'rate_limit_error',
      code: 429
    }
  });
}

res.status(error.response?.status || 500).json({
  error: error.response?.data || { 
    message: error.message || "Unknown error occurred",
    type: 'invalid_request_error'
  }
});
```

}
});

// ============================================================================
// ALTERNATIVE ROUTES
// ============================================================================

app.post(â€/v1/chat/completions/â€, (req, res, next) => {
req.url = â€œ/v1/chat/completionsâ€;
app.handle(req, res, next);
});

app.post(â€/chat/completionsâ€, (req, res, next) => {
req.url = â€œ/v1/chat/completionsâ€;
app.handle(req, res, next);
});

app.post(â€/â€, async (req, res) => {
recordHit(req);
if (req.body && req.body.messages) {
req.url = â€œ/v1/chat/completionsâ€;
return app.handle(req, res);
}
res.type(â€œtextâ€).send(â€œâœ… POST received! For chat, use /v1/chat/completionsâ€);
});

// ============================================================================
// CATCH-ALL 404
// ============================================================================

app.use((req, res) => {
recordHit(req);
console.log(â€œâŒ 404 - Route not found:â€, req.method, req.path);
res.status(404).json({
error: {
message: â€œRoute not foundâ€,
method: req.method,
path: req.path
}
});
});

// ============================================================================
// SERVER STARTUP
// ============================================================================

app.listen(PORT, () => {
console.log(â€=â€.repeat(60));
console.log(â€œğŸš€ OpenAI to NVIDIA NIM Proxyâ€);
console.log(â€   (Free Tier + Anti-Analyzing Fix)â€);
console.log(â€=â€.repeat(60));
console.log(`ğŸ“¡ Port:              ${PORT}`);
console.log(`ğŸ¤– Primary Model:     ${PRIMARY_MODEL}`);
console.log(`ğŸ”„ Fallback Models:   ${FALLBACK_MODELS.length} DeepSeek models`);
console.log(`ğŸ”‘ API Key:           ${API_KEY ? "âœ… Loaded" : "âŒ Missing"}`);
console.log(`ğŸ’¾ Max Request Size:  100MB`);
console.log(`â±ï¸  Request Timeout:   10 minutes`);
console.log(`ğŸ“‰ Smart Truncation:  ${ENABLE_SMART_TRUNCATION ? "ON (Adaptive)" : "OFF"}`);
console.log(`ğŸ›¡ï¸  Anti-Analyzing:    âœ… ENABLED`);
console.log(`ğŸŒ Health Check:      http://localhost:${PORT}/health`);
console.log(â€=â€.repeat(60));
});
